{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Formative Assessment: NLP - Emotion Classification in Text\n",
    "\n",
    "Objective: \n",
    "Develop machine learning models to classify emotions in text samples.\n",
    "\n",
    "Dataset:\n",
    "https://drive.google.com/file/d/1HWczIICsMpaL8EJyu48ZvRFcXx3_pcnb/view?usp=drive_link\n",
    "\n",
    "Key components to be fulfilled :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1. Loading and Preprocessing\n",
    "\n",
    "Load the dataset and perform necessary preprocessing steps. This should include text cleaning, tokenization, and removal of stopwords. Explain the preprocessing techniques used and their impact on model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment Emotion\n",
      "0  i seriously hate one subject to death but now ...    fear\n",
      "1                 im so full of life i feel appalled   anger\n",
      "2  i sit here to write i start to dig out my feel...    fear\n",
      "3  ive been really angry with r and i feel like a...     joy\n",
      "4  i feel suspicious if there is no one outside l...    fear\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "url = \"https://drive.google.com/uc?id=1HWczIICsMpaL8EJyu48ZvRFcXx3_pcnb\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Preprocessing Steps\n",
    "\n",
    "Text Cleaning  - This involves removing unwanted characters, special symbols, or numbers that donâ€™t contribute to the emotional meaning of the text.\n",
    "\n",
    "Text cleaning helps in normalizing the text, reducing variability, and making it easier for the model to learn patterns relevant to emotion classification.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment Emotion  \\\n",
      "0  i seriously hate one subject to death but now ...    fear   \n",
      "1                 im so full of life i feel appalled   anger   \n",
      "2  i sit here to write i start to dig out my feel...    fear   \n",
      "3  ive been really angry with r and i feel like a...     joy   \n",
      "4  i feel suspicious if there is no one outside l...    fear   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  i seriously hate one subject to death but now ...  \n",
      "1                 im so full of life i feel appalled  \n",
      "2  i sit here to write i start to dig out my feel...  \n",
      "3  ive been really angry with r and i feel like a...  \n",
      "4  i feel suspicious if there is no one outside l...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.lower()               # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "data['cleaned_text'] = data['Comment'].apply(clean_text)\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Tokenization\n",
    "\n",
    "This process splitting the cleaned text into individual words or tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "print(nltk.__version__)\n",
    "\n",
    "# Download the punkt resource\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment Emotion  \\\n",
      "0  i seriously hate one subject to death but now ...    fear   \n",
      "1                 im so full of life i feel appalled   anger   \n",
      "2  i sit here to write i start to dig out my feel...    fear   \n",
      "3  ive been really angry with r and i feel like a...     joy   \n",
      "4  i feel suspicious if there is no one outside l...    fear   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  i seriously hate one subject to death but now ...   \n",
      "1                 im so full of life i feel appalled   \n",
      "2  i sit here to write i start to dig out my feel...   \n",
      "3  ive been really angry with r and i feel like a...   \n",
      "4  i feel suspicious if there is no one outside l...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [i, seriously, hate, one, subject, to, death, ...  \n",
      "1        [im, so, full, of, life, i, feel, appalled]  \n",
      "2  [i, sit, here, to, write, i, start, to, dig, o...  \n",
      "3  [ive, been, really, angry, with, r, and, i, fe...  \n",
      "4  [i, feel, suspicious, if, there, is, no, one, ...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Tokenization function\n",
    "from nltk.tokenize import word_tokenize\n",
    "data['tokens'] = data['cleaned_text'].apply(word_tokenize)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Removing Stopwords\n",
    "\n",
    "Stopwords are common words that usually do not add significant meaning to a sentence (e.g., \"and\", \"the\", \"is\"). Removing them can improve model performance by reducing dimensionality and focusing on meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment Emotion  \\\n",
      "0  i seriously hate one subject to death but now ...    fear   \n",
      "1                 im so full of life i feel appalled   anger   \n",
      "2  i sit here to write i start to dig out my feel...    fear   \n",
      "3  ive been really angry with r and i feel like a...     joy   \n",
      "4  i feel suspicious if there is no one outside l...    fear   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  i seriously hate one subject to death but now ...   \n",
      "1                 im so full of life i feel appalled   \n",
      "2  i sit here to write i start to dig out my feel...   \n",
      "3  ive been really angry with r and i feel like a...   \n",
      "4  i feel suspicious if there is no one outside l...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [i, seriously, hate, one, subject, to, death, ...   \n",
      "1        [im, so, full, of, life, i, feel, appalled]   \n",
      "2  [i, sit, here, to, write, i, start, to, dig, o...   \n",
      "3  [ive, been, really, angry, with, r, and, i, fe...   \n",
      "4  [i, feel, suspicious, if, there, is, no, one, ...   \n",
      "\n",
      "                                     filtered_tokens  \n",
      "0  [seriously, hate, one, subject, death, feel, r...  \n",
      "1                   [im, full, life, feel, appalled]  \n",
      "2  [sit, write, start, dig, feelings, think, afra...  \n",
      "3  [ive, really, angry, r, feel, like, idiot, tru...  \n",
      "4  [feel, suspicious, one, outside, like, rapture...  \n"
     ]
    }
   ],
   "source": [
    "stop_words =set(stopwords.words('english'))\n",
    "data['filtered_tokens'] = data['tokens'].apply(lambda x:[word for word in x if word not in stop_words])\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the preprocessing techniques used and their impact on model performance.\n",
    "\n",
    "Lowercasing: Helps in standardizing words, ensuring that the same words in different cases (e.g., \"Happy\" vs. \"happy\") are treated equally.\n",
    "Removing punctuation and special characters: Reduces noise and focuses the model on meaningful content.\n",
    "Tokenization: Converts the text into a structured format that can be easily analyzed.\n",
    "Removing stopwords: Enhances model performance by reducing the input dimensionality and focusing on words that carry more emotional weight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Feature Extraction :\n",
    "\n",
    "Implement feature extraction using CountVectorizer or TfidfVectorizer. Describe how the chosen method transforms the text data into numerical features.\n",
    "\n",
    "\n",
    "Feature extraction is a crucial step in preparing text data for machine learning models. In this case, we can use either CountVectorizer or TfidfVectorizer from the sklearn.feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##CountVectorizer\n",
    "\n",
    "CountVectorizer converts a collection of text documents into a matrix of token counts. It essentially counts the occurrences of each word in the document and creates a sparse matrix where:\n",
    "\n",
    "Rows represent documents.\n",
    "Columns represent unique words (features).\n",
    "Values represent the count of each word in the respective document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aa  aac  aaron  ab  abandon  abandoned  abandonment  abbigail  abc  \\\n",
      "0   0    0      0   0        0          0            0         0    0   \n",
      "1   0    0      0   0        0          0            0         0    0   \n",
      "2   0    0      0   0        0          0            0         0    0   \n",
      "3   0    0      0   0        0          0            0         0    0   \n",
      "4   0    0      0   0        0          0            0         0    0   \n",
      "\n",
      "   abdomen  ...  zendikar  zero  zest  zhu  zipline  zombies  zone  \\\n",
      "0        0  ...         0     0     0    0        0        0     0   \n",
      "1        0  ...         0     0     0    0        0        0     0   \n",
      "2        0  ...         0     0     0    0        0        0     0   \n",
      "3        0  ...         0     0     0    0        0        0     0   \n",
      "4        0  ...         0     0     0    0        0        0     0   \n",
      "\n",
      "   zonisamide  zq  zumba  \n",
      "0           0   0      0  \n",
      "1           0   0      0  \n",
      "2           0   0      0  \n",
      "3           0   0      0  \n",
      "4           0   0      0  \n",
      "\n",
      "[5 rows x 8954 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#initialise countVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "#fit and transform cleaned text data\n",
    "x_count = count_vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "#convert to a dataframe for better visualization\n",
    "count_df = pd.DataFrame(x_count.toarray(), columns =count_vectorizer.get_feature_names_out() )\n",
    "\n",
    "print(count_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TfidfVectorizer\n",
    "\n",
    "TfidfVectorizer stands for Term Frequency-Inverse Document Frequency. It not only counts the word occurrences but also weighs them according to their importance across all documents. The idea is to down-weight common words and up-weight rarer words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    aa  aac  aaron   ab  abandon  abandoned  abandonment  abbigail  abc  \\\n",
      "0  0.0  0.0    0.0  0.0      0.0        0.0          0.0       0.0  0.0   \n",
      "1  0.0  0.0    0.0  0.0      0.0        0.0          0.0       0.0  0.0   \n",
      "2  0.0  0.0    0.0  0.0      0.0        0.0          0.0       0.0  0.0   \n",
      "3  0.0  0.0    0.0  0.0      0.0        0.0          0.0       0.0  0.0   \n",
      "4  0.0  0.0    0.0  0.0      0.0        0.0          0.0       0.0  0.0   \n",
      "\n",
      "   abdomen  ...  zendikar  zero  zest  zhu  zipline  zombies  zone  \\\n",
      "0      0.0  ...       0.0   0.0   0.0  0.0      0.0      0.0   0.0   \n",
      "1      0.0  ...       0.0   0.0   0.0  0.0      0.0      0.0   0.0   \n",
      "2      0.0  ...       0.0   0.0   0.0  0.0      0.0      0.0   0.0   \n",
      "3      0.0  ...       0.0   0.0   0.0  0.0      0.0      0.0   0.0   \n",
      "4      0.0  ...       0.0   0.0   0.0  0.0      0.0      0.0   0.0   \n",
      "\n",
      "   zonisamide   zq  zumba  \n",
      "0         0.0  0.0    0.0  \n",
      "1         0.0  0.0    0.0  \n",
      "2         0.0  0.0    0.0  \n",
      "3         0.0  0.0    0.0  \n",
      "4         0.0  0.0    0.0  \n",
      "\n",
      "[5 rows x 8954 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#initialise TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "#fit and transform cleaned text data\n",
    "\n",
    "x_tfidf = tfidf_vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(x_tfidf.toarray(), columns = tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. Model Development :\n",
    "\n",
    "Train the following machine learning models\n",
    "a)Naive Bayes\n",
    "b)Support Vector Machine \n",
    "\n",
    "\n",
    "To train machine learning models for emotion classification, we can use the Naive Bayes and Support Vector Machine (SVM) algorithms. Below, I'll outline the steps to develop and train these models using the features extracted with TfidfVectorizer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's split the data into training and testing sets:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming `X_tfidf` contains the features and `data['emotion']` is the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_tfidf, data['Emotion'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Naive Bayes Model\n",
    "\n",
    "\n",
    "Naive Bayes is a simple yet effective algorithm for text classification. We'll use the Multinomial Naive Bayes variant, which is suitable for discrete data like word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.90\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.87      0.94      0.90       392\n",
      "        fear       0.92      0.89      0.90       416\n",
      "         joy       0.91      0.88      0.90       380\n",
      "\n",
      "    accuracy                           0.90      1188\n",
      "   macro avg       0.90      0.90      0.90      1188\n",
      "weighted avg       0.90      0.90      0.90      1188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "#Train the naive bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train,y_train)\n",
    "\n",
    "#predict the test set\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "\n",
    "#evaluate the model\n",
    "\n",
    "accuracy_nb = accuracy_score(y_test,y_pred_nb)\n",
    "report_nb = classification_report(y_test,y_pred_nb)\n",
    "\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_nb:.2f}\")\n",
    "print(\"Classification Report:\\n\", report_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Support Vector Machine (SVM) Model\n",
    "SVM is another powerful algorithm for classification tasks. We'll use a linear kernel for this example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.94\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.91      0.94      0.92       392\n",
      "        fear       0.97      0.91      0.94       416\n",
      "         joy       0.93      0.96      0.94       380\n",
      "\n",
      "    accuracy                           0.94      1188\n",
      "   macro avg       0.94      0.94      0.94      1188\n",
      "weighted avg       0.94      0.94      0.94      1188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#train the SVM model\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train,y_train)\n",
    "\n",
    "#predict on the test set\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "#evaluate the model\n",
    "\n",
    "accuracy_svm = accuracy_score(y_test,y_pred_svm)\n",
    "report_svm = classification_report(y_test,y_pred_svm)\n",
    "\n",
    "print(f\"SVM Accuracy: {accuracy_svm:.2f}\")\n",
    "print(\"Classification Report:\\n\", report_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model outperforms the Naive Bayes model with an accuracy of 94% compared to 90%. This indicates that SVM is more effective for this specific task of emotion classification.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
