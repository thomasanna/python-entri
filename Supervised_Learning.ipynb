{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the breast cancer dataset from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
      "0                 0.07871  ...          17.33           184.60      2019.0   \n",
      "1                 0.05667  ...          23.41           158.80      1956.0   \n",
      "2                 0.05999  ...          25.53           152.50      1709.0   \n",
      "3                 0.09744  ...          26.50            98.87       567.7   \n",
      "4                 0.05883  ...          16.67           152.20      1575.0   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  target  \n",
      "0          0.4601                  0.11890       0  \n",
      "1          0.2750                  0.08902       0  \n",
      "2          0.3613                  0.08758       0  \n",
      "3          0.6638                  0.17300       0  \n",
      "4          0.2364                  0.07678       0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "#Load the breast cancer dataset from sklearn.\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Convert to a DataFrame for easier handling\n",
    "df = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data to handle any missing values and perform necessary feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each features mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "target                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values in each features\", missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since there are no missing values in this dataset,we can proceed with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first few rows of the scaled DataFrame\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0     1.097064     -2.073335        1.269934   0.984375         1.568466   \n",
      "1     1.829821     -0.353632        1.685955   1.908708        -0.826962   \n",
      "2     1.579888      0.456187        1.566503   1.558884         0.942210   \n",
      "3    -0.768909      0.253732       -0.592687  -0.764464         3.283553   \n",
      "4     1.750297     -1.151816        1.776573   1.826229         0.280372   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0          3.283515        2.652874             2.532475       2.217515   \n",
      "1         -0.487072       -0.023846             0.548144       0.001392   \n",
      "2          1.052926        1.363478             2.037231       0.939685   \n",
      "3          3.402909        1.915897             1.451707       2.867383   \n",
      "4          0.539340        1.371011             1.428493      -0.009560   \n",
      "\n",
      "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
      "0                2.255747  ...      -1.359293         2.303601    2.001237   \n",
      "1               -0.868652  ...      -0.369203         1.535126    1.890489   \n",
      "2               -0.398008  ...      -0.023974         1.347475    1.456285   \n",
      "3                4.910919  ...       0.133984        -0.249939   -0.550021   \n",
      "4               -0.562450  ...      -1.466770         1.338539    1.220724   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0          1.307686           2.616665         2.109526              2.296076   \n",
      "1         -0.375612          -0.430444        -0.146749              1.087084   \n",
      "2          0.527407           1.082932         0.854974              1.955000   \n",
      "3          3.394275           3.893397         1.989588              2.175786   \n",
      "4          0.220556          -0.313395         0.613179              0.729259   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  target  \n",
      "0        2.750622                 1.937015       0  \n",
      "1       -0.243890                 0.281190       0  \n",
      "2        1.152255                 0.201391       0  \n",
      "3        6.046041                 4.935010       0  \n",
      "4       -0.868353                -0.397100       0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "features = df.drop('target',axis=1)\n",
    "target = df['target']\n",
    "\n",
    "#perform feature scaling using standardscaler\n",
    "\n",
    "#Feature scaling is an essential preprocessing step in machine learning to ensure that all features contribute \n",
    "# equally to the model's performance. Scaling transforms features to have a specific range or distribution, \n",
    "# which can improve the convergence of gradient descent algorithms and the performance of distance-based models.\n",
    "\n",
    "# StandardScaler standardizes features by removing the mean and scaling to unit variance. \n",
    "# It transforms the data so that it has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "#convert scaled features back to a dataframe\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled_features,columns = features.columns)\n",
    "scaled_df['target'] = target.values\n",
    "\n",
    "# Display the first few rows of the scaled DataFrame\n",
    "\n",
    "print(\"the first few rows of the scaled DataFrame\")\n",
    "print(scaled_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the preprocessing steps you performed and justify why they are necessary for this dataset.\n",
    "\n",
    "1. Checking for missing values\n",
    "    We used df.isnull().sum() to check for missing values in dataset. Missing values can lead to inacuurate model training and predictions. Its important to identify them. In this case breast cancer dataset from scikit-learn does not containing missing values,simplifying the preprocessing steps\n",
    "\n",
    "2. Feature Scaling\n",
    "   We applied standardScaler to standardise the feature.\n",
    "   Normalization of scale - the features in the data set may have diffrent units and ranges.For example, some features might be in the range of 0 and 1000,while others are between 0 and 1.This discrepancy can affect the performance of algorithms that are sensitive to the scale of data,\n",
    "   pariculary distance based algorithms (like KNN),and gradient descent based algorithms(like logistic regression)\n",
    "\n",
    "   Improving convergence - Scaling helps algorithms converge more quickly during training by ensuring that the gradiants are not dominated by the largest features values\n",
    "\n",
    "   Equal importance - By scaling features to have a mean of 0 and standard deviation of 1, we treat all features equally , preventing the model from being biased towards features with larger magnitudes\n",
    "\n",
    "In summary, the breast cancer dataset does not require extensive preprocessing, these steps ensure that the data is clean and ready for effective model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Classification Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Logistic Regression\n",
    "\n",
    "-Logistic regression is a statistical method used primarily for binary classification problems, where the outcome variable is categorical and typically has two classes (e.g., success/failure, yes/no, 1/0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training test shapes (455, 30)\n",
      "Testing set shape (114, 30)\n"
     ]
    }
   ],
   "source": [
    "#split the dataset into training and testing sets\n",
    "x_train,x_test,y_train,y_test = train_test_split(scaled_features,target,test_size = 0.2,random_state = 42)\n",
    "\n",
    "# test_size = 0.2 means 20% of the data will be used for testing , while the remaining 80% for the training\n",
    "#Training vs. Testing: A larger training set typically helps in better model training, while a smaller testing set is \n",
    "# sufficient to evaluate the model's performance. The 80-20 split is a common practice, balancing the need for both training \n",
    "# and testing data\n",
    "\n",
    "#Setting random_state ensures that the results are reproducible. Every time you run the code with the same random state,\n",
    "#  you'll get the same split of the dataset into training and testing sets.\n",
    "\n",
    "#output shapes\n",
    "\n",
    "print(\"Training test shapes\", x_train.shape)\n",
    "print(\"Testing set shape\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression: 0.9736842105263158\n",
      "Confusion Matrix:\n",
      " [[41  2]\n",
      " [ 1 70]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logigistic regression model\n",
    "log_reg = LogisticRegression(max_iter=1000,random_state=42)\n",
    "log_reg.fit(x_train,y_train)\n",
    "\n",
    "#prediction \n",
    "y_pred= log_reg.predict(x_test)\n",
    "\n",
    "#evaluation\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "confusion = confusion_matrix(y_test,y_pred)\n",
    "report  = classification_report(y_test,y_pred)\n",
    "\n",
    "print(\"Accuracy of Logistic Regression:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the results from the Logistic Regression model .\n",
    "1. Accuracy\n",
    "   This model correctly classified approximately 97.37 %of the instances in the test set.This indicates the model preform well\n",
    "   overall in distinguishing between the two classes (malignant vs benign)\n",
    "\n",
    "2. Confusion matrix\n",
    "   \n",
    "   True Negatives (TN) - this model correctly predicted 41 benign case(class 0)\n",
    "   False Positives(FP) This model imcorrectly predicted 2 benign cases as malignant(class 1)\n",
    "   False Negatives(FN) - This model incorrectly predicted 1 malignant cases as benign(Class 0)\n",
    "   True Positives (TP) - This model corrctly predicted 70 Malignant cases(Class 1)\n",
    "\n",
    "3. Classification report\n",
    "\n",
    "   Class 0(Benign)\n",
    "       Precission - of all instances predicted, 98% were actualy benign. This indicates high level of accuracy in the benign predictions\n",
    "       Recall  - out of all actual benign cases, the model correctly identified 95% of them. This indicates the model missed 5% of the benign cases.\n",
    "       F1-score - This is the harmonic mean of the precision and recall. indicating a good balance between them\n",
    "    Class 1(Malignant)\n",
    "        Precision - of all instances predicted,97% were actually Malignant. This indicates high level of accuracy in the Malignant predictions\n",
    "        Recall  - Out of all actua; malignant cases, the model correctly identified 99%of them. This indicates the model missed 1% of the Malignat cases\n",
    "        F1-Score  - Good balance between precision and recall for this class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree Classifier: 0.9473684210526315\n",
      "Confusion Matrix:\n",
      " [[40  3]\n",
      " [ 3 68]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initialize descition tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "#fit the model on the training data\n",
    "\n",
    "dt_classifier.fit(x_train,y_train)\n",
    "\n",
    "#make predictions on test set\n",
    "y_pred_dt = dt_classifier.predict(x_test)\n",
    "\n",
    "#evaluate the model\n",
    "\n",
    "accuracy_dt = accuracy_score(y_test,y_pred_dt)\n",
    "confusion_dt = confusion_matrix(y_test,y_pred_dt)\n",
    "report_dt  = classification_report(y_test,y_pred_dt)\n",
    "\n",
    "print(\"Accuracy of Decision Tree Classifier:\", accuracy_dt)\n",
    "print(\"Confusion Matrix:\\n\", confusion_dt)\n",
    "print(\"Classification Report:\\n\", report_dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Decision Tree Classifier works by splitting the dataset into subsets based on the value of input features. Here's a simplified description of the process:\n",
    "\n",
    "Tree Structure:\n",
    "\n",
    "A decision tree consists of nodes, where each internal node represents a feature (or attribute), each branch represents a decision rule, and each leaf node represents an outcome (class label).\n",
    "Splitting:\n",
    "\n",
    "The algorithm recursively splits the data at each node based on feature values. The goal is to create branches that lead to pure nodes (i.e., nodes where all data points belong to a single class).\n",
    "Various criteria (such as Gini impurity or entropy) are used to determine the best feature and the best threshold for splitting the data.\n",
    "Stopping Criteria:\n",
    "\n",
    "The splitting process continues until a stopping criterion is met, such as reaching a maximum depth, having too few samples in a node, or achieving a certain level of purity.\n",
    "Prediction:\n",
    "\n",
    "Once the tree is constructed, making predictions involves traversing the tree from the root to a leaf node, following the decision rules based on the feature values of the input instance.\n",
    "\n",
    "\n",
    "Decision trees are easy to interpret and visualize. The decision-making process is straightforward, making it possible to understand how predictions are made. This is particularly useful in fields like healthcare, where stakeholders may need to understand the rationale behind model decisions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Classifier: 0.9649122807017544\n",
      "Confusion Matrix:\n",
      " [[40  3]\n",
      " [ 1 70]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95        43\n",
      "           1       0.96      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initialise random forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100,random_state=42)\n",
    "\n",
    "#fit the model on the teraining data\n",
    "rf_classifier.fit(x_train,y_train)\n",
    "\n",
    "#make predictions on the test set\n",
    "\n",
    "y_pred_rf = rf_classifier.predict(x_test)\n",
    "\n",
    "#evaluate the model\n",
    "accuracy_rf = accuracy_score(y_test,y_pred_rf)\n",
    "confusion_rf = confusion_matrix(y_test,y_pred_rf)\n",
    "report_rf = classification_report(y_test,y_pred_rf)\n",
    "\n",
    "print(\"Accuracy of Random Forest Classifier:\", accuracy_rf)\n",
    "print(\"Confusion Matrix:\\n\", confusion_rf)\n",
    "print(\"Classification Report:\\n\", report_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble learning method that builds multiple decision trees and merges their predictions to improve overall accuracy and robustness. Hereâ€™s a brief description of its functioning:\n",
    "\n",
    "Bootstrap Sampling:\n",
    "\n",
    "The algorithm generates multiple subsets of the original training data through a process called bootstrapping (random sampling with replacement). Each subset is used to train a separate decision tree.\n",
    "Decision Tree Creation:\n",
    "\n",
    "For each tree, a random subset of features is selected at each split, ensuring that different trees learn different aspects of the data. This randomness helps to decorrelate the trees, which improves the ensemble's performance.\n",
    "Voting Mechanism:\n",
    "\n",
    "Once all the trees are trained, predictions are made for a given input by aggregating the predictions of all the individual trees. For classification tasks, the final output is typically determined by majority voting (the class that receives the most votes from the trees).\n",
    "Feature Importance Calculation:\n",
    "\n",
    "After training, Random Forest can provide insights into feature importance, showing which features contribute most to the predictions.\n",
    "\n",
    "Random Forest is known for its ability to provide high accuracy while being robust against overfitting, especially when dealing with complex datasets. This is valuable for medical datasets where accurate predictions can significantly impact patient outcomes\n",
    "\n",
    "\n",
    "Overall, the Random Forest Classifier is a powerful and versatile tool well-suited for the breast cancer dataset due to its robustness, interpretability, and ability to model complex relationships. Its ensemble nature enhances prediction accuracy while providing valuable insights into feature importance, making it a strong candidate for classification tasks in healthcare.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Support Vector Machine (SVM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM) is a supervised learning algorithm commonly used for classification tasks. The core idea behind SVM is to find the optimal hyperplane that separates data points of different classes in a high-dimensional space. The \"support vectors\" are the data points that are closest to this hyperplane, and they are critical in defining its position and orientation\n",
    "\n",
    "Hyperplane: SVM attempts to find the hyperplane that best separates the classes. In two dimensions, this hyperplane is a line; in three dimensions, it's a plane; and in higher dimensions, it becomes a hyperplane.\n",
    "\n",
    "Maximizing Margin: SVM seeks to maximize the margin between the hyperplane and the nearest data points from either class (the support vectors). A larger margin often leads to better generalization on unseen data.\n",
    "\n",
    "Kernel Trick: SVM can efficiently perform non-linear classification using a technique called the kernel trick. This allows SVM to project data into higher dimensions where it becomes easier to separate classes with a hyperplane. Common kernels include linear, polynomial, and radial basis function (RBF).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Support Vector Machine: 0.97\n",
      "Confusion Matrix:\n",
      " [[41  2]\n",
      " [ 1 70]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train the SVM model\n",
    "svm_model = SVC(kernel='rbf',random_state=42)\n",
    "svm_model.fit(x_train,y_train)\n",
    "\n",
    "#make predictions\n",
    "y_pred_svm = svm_model.predict(x_test)\n",
    "\n",
    "#evaluate the model\n",
    "\n",
    "accuracy_svm = accuracy_score(y_test,y_pred_svm)\n",
    "conf_matrix_svm = confusion_matrix(y_test,y_pred_svm)\n",
    "class_report_svm = classification_report(y_test,y_pred_svm)\n",
    "\n",
    "print(f\"Accuracy of Support Vector Machine: {accuracy_svm:.2f}\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_svm)\n",
    "print(\"Classification Report:\\n\", class_report_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieved an accuracy of 0.97, indicating that 97% of the predictions made by the SVM classifier were correct. This is a strong result, suggesting that the model is effective at distinguishing between benign (0) and malignant (1)  in the breast cancer dataset\n",
    "\n",
    "High Precision and Recall:\n",
    "\n",
    "The model shows high precision and recall for both classes, especially for malignant tumors (class 1). This indicates that the model is reliable in identifying malignant tumors while minimizing false alarms (benign tumors classified as malignant).\n",
    "False Positives and False Negatives:\n",
    "\n",
    "The number of false positives (2) is very low, indicating that the model rarely mistakes benign tumors for malignant ones.\n",
    "The single false negative (1) suggests that the model occasionally misses a malignant tumor, which could be critical in a medical context.\n",
    "Balanced Performance:\n",
    "\n",
    "The macro and weighted averages of precision, recall, and F1-score indicate a balanced performance across both classes, with no significant bias toward one class over the other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. k-Nearest Neighbors (k-NN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: k-Nearest Neighbors (k-NN) is a simple, non-parametric, supervised learning algorithm used for classification and regression. The core idea of k-NN is to classify a data point based on the classes of its k nearest neighbors in the feature space. The distance between data points is typically measured using metrics like Euclidean distance.\n",
    "\n",
    "How k-NN Works:\n",
    "Choosing k: The user defines a value for k, which determines how many neighbors to consider when making a prediction. A smaller k can be noisy and sensitive to outliers, while a larger k smooths out predictions but may overlook local patterns.\n",
    "\n",
    "Distance Metric: The algorithm calculates the distance between the target point and all other points in the dataset using a specified distance metric (e.g., Euclidean, Manhattan).\n",
    "\n",
    "Voting Mechanism: Once the k nearest neighbors are identified, the algorithm assigns the class label that is most common among these neighbors (for classification) or averages their values (for regression).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of k-Nearest Neighbors: 0.95\n",
      "Confusion Matrix:\n",
      " [[40  3]\n",
      " [ 3 68]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train the K-NN model\n",
    "k=5 # you can experiment with diffrent values of k\n",
    "knn_model = KNeighborsClassifier(n_neighbors = k)\n",
    "\n",
    "knn_model.fit(x_train,y_train)\n",
    "\n",
    "#make predictions\n",
    "y_pred_knn = knn_model.predict(x_test)\n",
    "\n",
    "#evaluate the model\n",
    "accuracy_knn = accuracy_score(y_test,y_pred_knn)\n",
    "conf_matrix_knn = confusion_matrix(y_test,y_pred_knn)\n",
    "class_report_knn = classification_report(y_test,y_pred_knn)\n",
    "\n",
    "print(f\"Accuracy of k-Nearest Neighbors: {accuracy_knn:.2f}\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_knn)\n",
    "print(\"Classification Report:\\n\", class_report_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: The k-NN model achieved an accuracy of 0.95, indicating that 95% of the predictions made by the classifier were correct. This is a strong result, although slightly lower than the 97% accuracy achieved by the SVM model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. Model Comparison (2 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "Highest: Logistic Regression and SVM (0.97)\n",
    "Lowest: Decision Tree and k-NN (0.95)\n",
    "Confusion Matrices:\n",
    "\n",
    "Logistic Regression and SVM had the fewest misclassifications (only 3 errors each).\n",
    "Decision Tree and k-NN had more misclassifications (6 errors each).\n",
    "Precision:\n",
    "\n",
    "Logistic Regression and SVM excelled in precision for both classes, especially for benign tumors.\n",
    "The Decision Tree and k-NN performed slightly lower in precision for benign tumors.\n",
    "Recall:\n",
    "\n",
    "Logistic Regression and SVM had a strong recall, particularly for malignant tumors (1), indicating they correctly identified almost all malignant cases.\n",
    "The Decision Tree and k-NN had a lower recall for benign tumors.\n",
    "F1-Score:\n",
    "\n",
    "Logistic Regression and SVM achieved the highest F1-scores, indicating a good balance between precision and recall.\n",
    "Decision Tree and k-NN had lower F1-scores, indicating some trade-offs between precision and recall.\n",
    "\n",
    "\n",
    "Overall, the results suggest that Logistic Regression and SVM may be the most reliable choices for classifying breast cancer in this dataset, but Random Forest is also a strong contender\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
